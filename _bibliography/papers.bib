@string{aps = {American Physical Society,}}

@inproceedings{jeong-etal-2022-kold,
    abbr = "EMNLP",
    title = "KOLD: Korean Offensive Language Dataset",
    author = "Jeong, Younghoon  and
      Oh, Juhyun  and
      Lee, Jongwon  and
      Ahn, Jaimeen and
      Moon, Jihyung and
      Park, Sungjoon and
      Oh, Alice",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pdf = "https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.744/",
    paper = "https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.744/",
    pages = "10818–10833",
    abstract = "Recent directions for offensive language detection are hierarchical modeling, identifying the type and the target of offensive language, and interpretability with offensive span annotation and prediction. These improvements are focused on English and do not transfer well to other languages because of cultural and linguistic differences. In this paper, we present the Korean Offensive Language Dataset (KOLD) comprising 40,429 comments, which are annotated hierarchically with the type and the target of offensive language, accompanied by annotations of the corresponding text spans. We collect the comments from NAVER news and YouTube platform and provide the titles of the articles and videos as the context information for the annotation process. We use these annotated comments as training data for Korean BERT and RoBERTa models and find that they are effective at offensiveness detection, target classification, and target span detection while having room for improvement for target group classification and offensive span detection. We discover that the target group distribution differs drastically from the existing English datasets, and observe that providing the context information improves the model performance in offensiveness detection (+0.3), target classification (+1.5), and target group classification (+13.1). We publicly release the dataset and baseline models.",
}

@inproceedings{cho-etal-2022-assessing,
    abbr = "AACL-IJCNLP",
    title = "Assessing How Users Display Self-Disclosure and Authenticity in Conversation with Human-Like Agents: A Case Study of Luda Lee",
    author = "Cho, Won Ik  and
      Kim, Soomin  and
      Choi, Eujeong  and
      Jeong, Younghoon",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2022.findings-aacl.14",
    pages = "145--152",
    abstract = "There is an ongoing discussion on what makes humans more engaged when interacting with conversational agents. However, in the area of language processing, there has been a paucity of studies on how people react to agents and share interactions with others. We attack this issue by investigating the user dialogues with human-like agents posted online and aim to analyze the dialogue patterns. We construct a taxonomy to discern the users{'} self-disclosure in the dialogue and the communication authenticity displayed in the user posting. We annotate the in-the-wild data, examine the reliability of the proposed scheme, and discuss how the categorization can be utilized for future research and industrial development.",
}

@inproceedings{cho-etal-2022-evaluating,
    abbr = "CODI",
    title = "Evaluating How Users Game and Display Conversation with Human-Like Agents",
    author = "Cho, Won Ik  and
      Kim, Soomin  and
      Choi, Eujeong  and
      Jeong, Younghoon",
    booktitle = "Proceedings of the 3rd Workshop on Computational Approaches to Discourse",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea and Online",
    publisher = "International Conference on Computational Linguistics",
    pdf = "https://aclanthology.org/2022.codi-1.3",
    pages = "19--27",
    abstract = "Recently, with the advent of high-performance generative language models, artificial agents that communicate directly with the users have become more human-like. This development allows users to perform a diverse range of trials with the agents, and the responses are sometimes displayed online by users who share or show-off their experiences. In this study, we explore dialogues with a social chatbot uploaded to an online community, with the aim of understanding how users game human-like agents and display their conversations. Having done this, we assert that user postings can be investigated from two aspects, namely conversation topic and purpose of testing, and suggest a categorization scheme for the analysis. We analyze 639 dialogues to develop an annotation protocol for the evaluation, and measure the agreement to demonstrate the validity. We find that the dialogue content does not necessarily reflect the purpose of testing, and also that users come up with creative strategies to game the agent without being penalized.",
}

@inproceedings{park2021klue,
    abbr = "NeurIPS",
    title={{KLUE}: Korean Language Understanding Evaluation},
    author={Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Ji Yoon Han and Jangwon Park and Chisung Song and Junseong Kim and Youngsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jung-Woo Ha and Kyunghyun Cho},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
    year={2021},
    pdf={https://openreview.net/forum?id=q-8h8-LZiUm},
    abstract={We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE is a collection of eight Korean natural language understanding (NLU) tasks, including Topic Classification, Semantic Textual Similarity, Natural LanguageInference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking. We create all of the datasets from scratch in a principled way. We design the tasks to have diverse formats and each task to be built upon various source corpora that respect copyrights. Also, we propose suitable evaluation metrics and organize annotation protocols in a way to ensure quality. To prevent ethical risks in KLUE, we proactively remove examples reflecting social biases, containing toxic content or personally identifiable information (PII). Along with the benchmark datasets, we release pre-trained language models (PLM) for Korean, KLUE-BERT and KLUE-RoBERTa, and find KLUE-Roberta-large outperforms other baselines including multilingual PLMs and existing open-source Korean PLMs. The fine-tuning recipes are publicly open for anyone to reproduce our baseline result. We believe our work will facilitate future research on cross-lingual as well as Korean language models and the creation of similar resources for other languages. KLUE is available at https://klue-benchmark.com.}
}

@inproceedings{jeong-etal-2021-automatic,
    abbr = "CLSW",
    author="Jeong, Younghoon
      and Li, Ming Yue
      and Kang, Su Min
      and Eum, Yun Kyung
      and Kang, Byeong Kwu",
    title="Automatic Prediction and Linguistic Interpretation of Chinese Directional Complements Based on BERT Model",
    booktitle="Chinese Lexical Semantics Workshop",
    year="2021",
    month="mar",
    publisher="Springer International Publishing",
    pages="405--416",
    abstract="The Chinese directional complement is one of the trickiest concepts for second language learners due to their derivative meanings. In particular, 出来, 起来, 下来, and 下去 are easy to be confused. This study aims to gain grammatical and educational insights for these complements with a neural network model. This study fine-tuned the Chinese BERT model with Chinese directional complement data composed of sentences containing the above four components used in literature, media, and textbook. By measuring these fine-tuned models' accuracy, we show how accurately and efficiently the neural network model predicts Chinese directional complements. Furthermore, we interpret and analyze the model's decision using the Sampling and Occlusion algorithm and visually present which components of the sentence influence the choice for complements.",
    pdf={https://link.springer.com/chapter/10.1007/978-3-031-06703-7_31}
}

@inproceedings{jeong-etal-2021-tom,
    abbr = "DSTC9@AAAI",
    author = "Jeong, Younghoon
      and Lee, Sejin
      and Seo, Jungyun",
    title = "TOM: End-to-End Task-Oriented Multimodal Dialog System with GPT-2",
    booktitle = "Proceedings of the AAAI Workshop: 9th Dialog System Technology Challenge",
    month = "jan",
    year = "2021",
    pdf = "https://drive.google.com/file/d/1_-F_7mGcmmIm7S5IyH-puVZM5uelp7nu/view"
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  selected={true}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.,},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}
